{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perceptron\n",
    "\n",
    "Lo primero que vamos a hacer es contruir un modelo supersencillo de clasificación empleando para ello un perceptrón tal y como lo vimos en las clases de teoría y que corresponde al esquema siguiente: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/perceptron-6168423.jpg\" alt=\"drawing\" width=\"650\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerda que nuestras unidades o neuronas artificiales tienen una forma similar un conjunto de entradas que están multiplicadas por unos pesos y que todo se suma y se pasa luego por una función de activación, que en el caso de la figura es lo que se denomina una step function y que nosotros denominamos pulso en castellano. La salida $y$ del perceptrón de la figura sería la siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z = w_0 + x_1 \\cdot w_1 + x_2 \\cdot w_2 + ... + x_n \\cdot w_n \\text { , donde z es la salida antes de la función de activación}$$\n",
    "$$ y = \n",
    "\\begin{cases} \n",
    "1 & \\text{si } z > 0 \\\\\n",
    "0 & \\text{de lo contrario}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repasado los conceptos vamos con el modelado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos cargando librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTA: Las implementaciones de sklearn de RRNN no soportan uso de GPU\n",
    "Tampoco podemos cambiar las funciones de activacion ni pesos iniciales en CADA capa.\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos datos. Utilizaremos el dataset de pinguinos de seaborn, conocido como \"penguins\".  \n",
    "\n",
    "Es un conjunto de datos que contiene mediciones de tres especies diferentes de pingüinos encontrados en las islas cercanas a la Antártida. Las especies incluidas son: Adelie, Chinstrap y Gentoo. Este dataset se ha vuelto popular como una alternativa al clásico dataset de Iris para tareas de clasificación y visualización de datos en el aprendizaje automático.\n",
    "\n",
    "Las variables presentes en el dataset incluyen:\n",
    "\n",
    "- **species**: La especie del pingüino (Adelie, Chinstrap, Gentoo).\n",
    "- **island**: La isla en la Antártida donde se observó el pingüino (Biscoe, Dream, o Torgersen).\n",
    "- **bill_length_mm**: La longitud del pico del pingüino en milímetros.\n",
    "- **bill_depth_mm**: La profundidad del pico del pingüino en milímetros.\n",
    "- **flipper_length_mm**: La longitud de la aleta del pingüino en milímetros.\n",
    "- **body_mass_g**: La masa corporal del pingüino en gramos.\n",
    "- **sex**: El sexo del pingüino.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos y primer vistazo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>island_Biscoe</th>\n",
       "      <th>island_Dream</th>\n",
       "      <th>island_Torgersen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
       "0        0            39.1           18.7              181.0       3750.0   \n",
       "1        0            39.5           17.4              186.0       3800.0   \n",
       "2        0            40.3           18.0              195.0       3250.0   \n",
       "4        0            36.7           19.3              193.0       3450.0   \n",
       "5        0            39.3           20.6              190.0       3650.0   \n",
       "\n",
       "   sex  island_Biscoe  island_Dream  island_Torgersen  \n",
       "0    0          False         False              True  \n",
       "1    1          False         False              True  \n",
       "2    1          False         False              True  \n",
       "4    1          False         False              True  \n",
       "5    0          False         False              True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sns.load_dataset(\"penguins\")\n",
    "\n",
    "# Limpiamos un poco los datos\n",
    "df.dropna(inplace=True) # Quitamos nulos\n",
    "cleanup_nums = {\"species\": {\"Adelie\": 0,\n",
    "                            \"Chinstrap\": 1,\n",
    "                            \"Gentoo\": 2},\n",
    "               \"sex\": {\"Male\": 0,\n",
    "                       \"Female\": 1}}\n",
    "df.replace(cleanup_nums, inplace=True) # Siempre aprendiendo trucos nuevos\n",
    "df = pd.get_dummies(df) # del manual:  If columns is None then all the columns with object, string, or category dtype will be converted. columns está a None por defecto\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 333 entries, 0 to 343\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   species            333 non-null    int64  \n",
      " 1   bill_length_mm     333 non-null    float64\n",
      " 2   bill_depth_mm      333 non-null    float64\n",
      " 3   flipper_length_mm  333 non-null    float64\n",
      " 4   body_mass_g        333 non-null    float64\n",
      " 5   sex                333 non-null    int64  \n",
      " 6   island_Biscoe      333 non-null    bool   \n",
      " 7   island_Dream       333 non-null    bool   \n",
      " 8   island_Torgersen   333 non-null    bool   \n",
      "dtypes: bool(3), float64(4), int64(2)\n",
      "memory usage: 19.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "333 instancias con 8 features no es un dataset de DL, pero para este ejercicio de fijación de conceptos nos vale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividimos en train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.iloc[:, 1:] # Otra formas de quitar el target\n",
    "y = df.iloc[:, 0] # y de quedárselo\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266, 8)\n",
      "(67, 8)\n",
      "(266,)\n",
      "(67,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la distribución del target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species\n",
       "0    0.438438\n",
       "2    0.357357\n",
       "1    0.204204\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.species.value_counts(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desbalanceado en favor de la clase 0 y 2 en detrimento de la 1. Habrá que tenerlo en cuenta. Ahora vendría normalizar e incluso siendo tan pequeño podríamos hacer hasta un miniEda, pero vamos a ir en plan DL full mode y vamos directo a entrenar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un detalle, el análisis del target del train siempre hay que hacerlo (aunque sea DL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando el Perceptrón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tan simple como importar desde sklearn y sin tocar nada..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34328358208955223"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Algoritmo de clasificación. El score es el accuracy.\n",
    "Sale muy malo, ¿el perceptron no es capaz de diferenciar las clases?\n",
    "'''\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X_train, y_train)\n",
    "per_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El perceptrón de primeras sólo sirve para clasificar y por tanto su score es un score de accuracy. Podemo ver su report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        31\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.34      1.00      0.51        23\n",
      "\n",
      "    accuracy                           0.34        67\n",
      "   macro avg       0.11      0.33      0.17        67\n",
      "weighted avg       0.12      0.34      0.18        67\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, per_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un desastre que curiosamente se ha centrado en la clase 2, la minoritaria..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9962406015037594\n",
      "0.9850746268656716\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Una simple regresión logistica las diferencia mejor\n",
    "'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=5000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "print(log_reg.score(X_train, y_train))\n",
    "print(log_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien está claro que así no vamos muy lejos, pero recuerda que **deberíamos haber escalado**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "minmax.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos empleado el minmax porque tenemos variables categóricas con valores 0,1 y así igualamos todas las escalas. Pero podría haber usado un `StandarScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled = minmax.transform(X_train)\n",
    "X_test_scaled = minmax.transform(X_test)\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X_train_scaled, y_train)\n",
    "per_clf.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el escalado todo ha cambiado. Aún así, imaginemos que no hubiera salido bien, tendríamos que probar con configuraciones más complejas como es por ejemplo el uso de una red densa como un MLP. Lo vemos en la siguientes sesión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta vez vamos a emplear un MLP muy sencillo con una única capa oculta de 100 neuronas. Lo haremos con sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier # Fijate que sklearn sí que distingue entre clasificador y regresor. No tendremos que hacerlo nosotros.\n",
    "from sklearn.neural_network import MLPRegressor # TB existe. No se usa en este ejemplo\n",
    "\n",
    "'''\n",
    "Por defecto una capa con 100 neuronas\n",
    "3 capas: input, una hidden y output\n",
    "'''\n",
    "mlp = MLPClassifier(max_iter=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos así sin tocar ningún hiperparametro, ¿qué estamos haciendo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 'auto',\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 50,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': None,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': False,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahí lo tienes. En concreto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topologícamente: Una capa oculta con 100 neuronas, cuya función de activación es \"RELU\", con batch_size a \"auto\" que quiere decir que estamos usando 200 instancias (este no nos gusta), learning rate a 0.001 y constante (usa un Adam de optimizador con momento y nesterov, ya lo trataremos con más cariño) y algunos hiperparámetros más que iremos tratando a medida  que avancemos más. Pero vemos que tenemos mucho juego a pesar de no ser  de lo más famosos de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=50, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenemos y comprobemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.650375939849624\n",
      "0.6716417910447762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(mlp.score(X_train, y_train))\n",
    "print(mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué tal? No sé como habrá ido, pero vamos a probar un par de veces más a ver qué ocurre con los resultados..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probablemente estes viendo que no son nada estables..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de intentar mejorarlo igual que hicimos con su hermano pequeño el perceptrón vamos a construirnos nuestra propia red usando las posibilidades de este MLPClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos otra configuración. Es posible crear una red neuronal desde la propia función de MLPClassifier()  \n",
    "Construyamos la siguiente red  \n",
    "<img src=\"./img/mi_primer_dl.png\" alt=\"drawing\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6578947368421053\n",
      "0.5970149253731343\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(max_iter=500,\n",
    "                   activation='tanh',\n",
    "                   hidden_layer_sizes = (150, 150, 150))\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(mlp.score(X_train, y_train))\n",
    "print(mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaya sigue igual, es hora de aplicar lo que ya debemos saber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scal =scaler.transform(X_train)\n",
    "X_test_scal =scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(max_iter=500)\n",
    "log_reg.fit(X_train_scal, y_train)\n",
    "print(log_reg.score(X_train_scal, y_train))\n",
    "print(log_reg.score(X_test_scal, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(max_iter=500)\n",
    "mlp.fit(X_train_scal, y_train)\n",
    "print(mlp.score(X_train_scal, y_train))\n",
    "print(mlp.score(X_test_scal, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es otro diferente con el escalado (y parece que el Standar o el MinMax, la normalización, resutan igual de potentes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cualquier caso, así tal cual no hemos podido tocar mucho más, para MLP relativamente sencillos el de sklearn debería valernos, peor  si queremos de verdad flexibilidad, entonces debemos acudir a otro framework como es Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
