{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APLICACIONES ML (II): Procesamiento de Lenguaje Natural (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El departamento de marketing de un sistema de streaming (Showtimes) quiere clasificar automáticamente las reviews de los usuarios de su servicio fremium en el que dejan que estos puedan ver parte de su catálogo a cambio de que hagan valoraciones de las películas y series que ven. En concreto están interesados en saber hasta que punto una review es positiva o negativa a partir de un dataset etiquetado por expertos que han revisado miles de reviews (25K) y las han etiquetado como positivas o negativas. Ahora quieren tener un sistema de ayuda al etiquetado y de preclasificación.  \n",
    "\n",
    "En definitiva, sobre un dataset que ya tienen trabajado quieren que les creemos un modelo que dada una review la clasifique en positiva o negativa. Pero sobre todo están interesados en que su sistema detecte las reviews negativas al mayor porcentaje posible para estar a tiempo de reaccionar con el contenido. Una vez clasificadas como negativas pasarían a un segundo nivel de revisión pero no quieren dejar pasarlas y tampoco sobrecargar este sistema por lo que a pesar de querer una detección alta, necesitan que el sistema no se equivoque más de un 30% cuando diga que una review es negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso I: Entendiendo el problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, entender el problema es:\n",
    "1. Saber que se trataría de un aprendizaje supervisado (datos etiquetados en positivas y negativas)\n",
    "2. Saber que es un clasficador binario.\n",
    "3. Saber que tendremos que tratar texto en lenguaje natural (las reviews) y no formal, ni escrito literariamente.\n",
    "4. Entender que la métrica que se pide es el recall de las reviews negativa con una precision de más de un 70% de dicha clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso II: Obtención de los datos y primer vistazo (y split train y test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de obtener los datos, vamos a hacer nuestra carga de librerías y modulos necesarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bootcampviztools as bt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "#import osm\n",
    "#import reb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a cargar el dataset del modulo de datasets que tiene Python, simulando que es Showtimes la que nos envía la info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "reviews_ds = load_dataset(\"imdb\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ds[\"text\"][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ds[\"label\"][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a transformarlo en un dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame( {\"reseña\":reviews_ds[\"text\"], \"valoración\": reviews_ds[\"label\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos con el vistazo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reseña</th>\n",
       "      <th>valoración</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              reseña  valoración\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...           0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...           0\n",
       "2  If only to avoid making this type of film in t...           0\n",
       "3  This film was probably inspired by Godard's Ma...           0\n",
       "4  Oh, brother...after hearing about this ridicul...           0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   reseña      25000 non-null  object\n",
      " 1   valoración  25000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 390.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bueno, pues este es uno de los \"problemas\" o trabajos en los que sólo tenemos texto en lenguaje natural como fuente de información (salvo CatBoost, en el momento de escribir esto), no hay modelo (no Deep) que lo soporte tal cual. Esto es nuestra actual feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.']\n"
     ]
    }
   ],
   "source": [
    "print(df.head(1)[\"reseña\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte III: Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df, test_size = 5000, stratify = df[\"valoración\"], random_state = 42)\n",
    "train_set.reset_index(inplace = True)\n",
    "test_set.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "valoración\n",
       "1    0.5\n",
       "0    0.5\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[\"valoración\"].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "valoración\n",
       "1    0.5\n",
       "0    0.5\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[\"valoración\"].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"valoración\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte IV: MiniEDA específico:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero veamos el target y luego iremos a la parte \"específica\" de preparación de features de un ML sobre NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bt.pinta_distribucion_categoricas(train_set, [target], mostrar_valores= True, relativa= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya lo sabíamos del value_counts, y hazle una foto porque como este no vamos a ver un dataset muchas veces más. Completamente equilibrado, \"accuracy\" nos vale como métrica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte IV.1: Extracción de Features: Vectorización de Textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siempre que tengamos un tipo de dato no estructurado (no tabular en general, como imágenes o textos) tendremos que entrar en esta parte, cómo obtener sus features como traducirlo en algún tipo de vector que los modelos puedan manejar (en las imágenes ya hemos hecho una vectorización, convertir cada imagen en una instancia cuyas features son el valor de color o gris de cada uno de sus píxeles puestos uno detrás de otro).  \n",
    "\n",
    "Ahora vamos a aprender un par de formas de vectorizar un texto es decir convertirlo en un vector de features que ya si podremos dar de \"comer\" a un modelo y tratarlas como hemos tratado cualquier feature hasta ahora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a seguir los siguientes pasos:\n",
    "1. Limpieza y tokenizacion\n",
    "2. Vectorización texto (I): Bag of Words (BOW) binario y no binario\n",
    "3. Vectorización texto (II): Tf-idf (term frequency inverse document frequency)\n",
    "4. Vectorización texto (IV): Usando n-gramas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpiamos (y tokenizamos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBJETIVO: Dejar las palabras/tokens y quitar todo lo que no sea eso\n",
    "Limpiar en procesamiento de texto básicamente consiste en quitar del texto todos los elementos que no aportan (signos de puntuación) para reducir el texto al conjunto de palabras/tokens que sí consideramos interesantes.  \n",
    "A) En general vamos a quitar dos cosas:\n",
    "1. Signos de puntuación y caracteres que no son texto _[Ojo: Tener en cuenta las dependencias con el idioma]_\n",
    "2. URLs, hashtags, emojis, etc\n",
    "3. Stopwords (palabras que en un idioma se repiten mucho, o tienen un significado gramatical y no léxico)\n",
    "\n",
    "Puede haber situaciones especiales en las que quiera conservar algunos de los elementos anteriores (por ejemplo los hashtags o los emojis si estoy procesando mensajes de redes sociales)\n",
    "\n",
    "B) Pasaremos todo a minúsculas (también depende de si quiero extraer Nombres Propios, entonces este sería mi último paso antes de generar el dataset final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limpieza básica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\") # Si volvemos a las expresiones regulares, :-), en texto no queda otra si no eres un LLM\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br \\s*/><br\\s*/>)|(\\-)|(\\/)|(_)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "\n",
    "def clean(row):\n",
    "    # Limpio signos y convierto a minúsculas\n",
    "    dato = REPLACE_NO_SPACE.sub(NO_SPACE, row.lower()) # Equivale a re.sub(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\", NO_SPACE, row.lower())\n",
    "    # Convierto los retornos de carro <br /><br /> en espacios y los guiones (\"-\")\n",
    "    dato = REPLACE_WITH_SPACE.sub(SPACE, dato) # Equivale a re.sub(\"(<br \\s*/><br\\s*/>)|(\\-)|(\\/)|(_)\", SPACE, dato)\n",
    "    # Quito cualquier link\n",
    "    dato = \" \".join([word for word in dato.split() if \"http\" not in word])\n",
    "    # Quito los stopwords\n",
    "    return dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"reseña_limpia\"] = train_set[\"reseña\"].apply(clean)\n",
    "test_set[\"reseña_limpia\"] = test_set[\"reseña\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have always been a huge James Bond fanatic! I have seen almost all of the films except for Die Another Day, and The World Is Not Enough. The graphic's for Everything Or Nothing are breathtaking! The voice talents......... WOW! I LOVE PIERCE BROSNAN! He is finally Bond in a video game! HE IS BOND! I enjoyed the past Bond games: Goldeneye, The World Is Not Enough, Agent Under Fire, and Nightfire. This one is definitely the best! Finally, Mr. Brosnan, (may I call him Mr. Brosnan as a sign of respect? Yes I can!) He was phenomenally exciting to hear in a video game....... AT LONG LAST! DUH! I've seen him perform with Robin Williams, and let me tell you, they make a great team. Pierce Brosnan is funny, wickedly handsome ( I mean to say wickedly in a good way,) and just one of those actor's who you would want to walk up to and wrap your arms around and hug, saying: \"Pierce Brosnan, thank you for being James Bond,\" \"If it wasn't for you, I wouldn't know who James Bond is.\" He's a great actor! I am a huge fan of Willem Dafoe even though I've seen him in a couple of movies. His role as Nikolai Diavalo was brilliant. (Did I spell the character's name right?) LOL!!!! He does a great job with an accent. Sometimes I can't even hear an accent. I have seen Willem, I mean Mr. Dafoe, perform in two movies: Finding Nemo, and Spider-Man with my favorite actress: KIRSTEN DUNST! SHE ROCKS! Anyway, He never ceases to amaze. And Richard Kiel, wow, he's definitely got the part of Jaw's nailed. I've seen him in the movie's and he's awesome! As a matter of fact, my Grandparent's have met Mr. Kiel, and I was jealous when they told me. But, Kirsten Dunst is at the top of my list of Celebritie's that I want to meet. John Cleese was breathtaking. I have never seen a better person play as the wisecracking, and gadget creating Q! Mr. Cleese was hilarious! I've seen him work with Pierce Brosnan in Goldeneye and Tommorow Never Dies. He's awesome! John Cleese's most recent project is Shrek 2 starring Mike Myer's, Cameron Diaz, Julie Andrew's and Eddie Murphy. ( Shrek 2 is now in theatre's!) GOOD LUCK 007! Oh, yeah, and as Q alway's says: \"Grow up 007!\"\n"
     ]
    }
   ],
   "source": [
    "print(train_set.loc[0,\"reseña\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have always been a huge james bond fanatic i have seen almost all of the films except for die another day and the world is not enough the graphic's for everything or nothing are breathtaking the voice talents wow i love pierce brosnan he is finally bond in a video game he is bond i enjoyed the past bond games goldeneye the world is not enough agent under fire and nightfire this one is definitely the best finally mr brosnan may i call him mr brosnan as a sign of respect yes i can he was phenomenally exciting to hear in a video game at long last duh i've seen him perform with robin williams and let me tell you they make a great team pierce brosnan is funny wickedly handsome i mean to say wickedly in a good way and just one of those actor's who you would want to walk up to and wrap your arms around and hug saying pierce brosnan thank you for being james bond if it wasn't for you i wouldn't know who james bond is he's a great actor i am a huge fan of willem dafoe even though i've seen him in a couple of movies his role as nikolai diavalo was brilliant did i spell the character's name right lol he does a great job with an accent sometimes i can't even hear an accent i have seen willem i mean mr dafoe perform in two movies finding nemo and spider man with my favorite actress kirsten dunst she rocks anyway he never ceases to amaze and richard kiel wow he's definitely got the part of jaw's nailed i've seen him in the movie's and he's awesome as a matter of fact my grandparent's have met mr kiel and i was jealous when they told me but kirsten dunst is at the top of my list of celebritie's that i want to meet john cleese was breathtaking i have never seen a better person play as the wisecracking and gadget creating q mr cleese was hilarious i've seen him work with pierce brosnan in goldeneye and tommorow never dies he's awesome john cleese's most recent project is shrek starring mike myer's cameron diaz julie andrew's and eddie murphy shrek is now in theatre's good luck oh yeah and as q alway's says grow up\n"
     ]
    }
   ],
   "source": [
    "print(train_set.loc[0,\"reseña_limpia\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopwords**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are the very common words like ‘if’, ‘but’, ‘we’, ‘he’, ‘she’, and ‘they’. We can usually remove these words without changing the semantics of a text and doing so often (but not always) improves the performance of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las palabras \"vacías\" o `stopwords` (en terminología NLP) son palabras muy comunes como 'si', 'pero', 'porque', 'y' (en inglés, otras como  ‘if’, ‘but’, ‘we’, ‘he’, ‘she’, y ‘they’), que no aportan significado léxico o semántico y que, generalmente podemos eliminar estas palabras sin cambiar la semántica de un texto y hacerlo a menudo (pero no siempre) mejora el rendimiento de un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sí, para cada idioma necesito unas stopwords, un diccionario particularizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"spanish\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = stopwords.words(\"english\")\n",
    "def remove_stopwords(row):\n",
    "    dato = \" \".join([word for word in row.split(\" \") if word not in dictionary])\n",
    "    return dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"reseña_limpia_sin_stopwords\"] = train_set[\"reseña_limpia\"].apply(remove_stopwords)\n",
    "test_set[\"reseña_limpia_sin_stopwords\"] = test_set[\"reseña_limpia\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have always been a huge james bond fanatic i have seen almost all of the films except for die another day and the world is not enough the graphic's for everything or nothing are breathtaking the voice talents wow i love pierce brosnan he is finally bond in a video game he is bond i enjoyed the past bond games goldeneye the world is not enough agent under fire and nightfire this one is definitely the best finally mr brosnan may i call him mr brosnan as a sign of respect yes i can he was phenomenally exciting to hear in a video game at long last duh i've seen him perform with robin williams and let me tell you they make a great team pierce brosnan is funny wickedly handsome i mean to say wickedly in a good way and just one of those actor's who you would want to walk up to and wrap your arms around and hug saying pierce brosnan thank you for being james bond if it wasn't for you i wouldn't know who james bond is he's a great actor i am a huge fan of willem dafoe even though i've seen him in a couple of movies his role as nikolai diavalo was brilliant did i spell the character's name right lol he does a great job with an accent sometimes i can't even hear an accent i have seen willem i mean mr dafoe perform in two movies finding nemo and spider man with my favorite actress kirsten dunst she rocks anyway he never ceases to amaze and richard kiel wow he's definitely got the part of jaw's nailed i've seen him in the movie's and he's awesome as a matter of fact my grandparent's have met mr kiel and i was jealous when they told me but kirsten dunst is at the top of my list of celebritie's that i want to meet john cleese was breathtaking i have never seen a better person play as the wisecracking and gadget creating q mr cleese was hilarious i've seen him work with pierce brosnan in goldeneye and tommorow never dies he's awesome john cleese's most recent project is shrek starring mike myer's cameron diaz julie andrew's and eddie murphy shrek is now in theatre's good luck oh yeah and as q alway's says grow up\n"
     ]
    }
   ],
   "source": [
    "print(train_set.loc[0,\"reseña_limpia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "always huge james bond fanatic seen almost films except die another day world enough graphic's everything nothing breathtaking voice talents wow love pierce brosnan finally bond video game bond enjoyed past bond games goldeneye world enough agent fire nightfire one definitely best finally mr brosnan may call mr brosnan sign respect yes phenomenally exciting hear video game long last duh i've seen perform robin williams let tell make great team pierce brosnan funny wickedly handsome mean say wickedly good way one actor's would want walk wrap arms around hug saying pierce brosnan thank james bond know james bond he's great actor huge fan willem dafoe even though i've seen couple movies role nikolai diavalo brilliant spell character's name right lol great job accent sometimes can't even hear accent seen willem mean mr dafoe perform two movies finding nemo spider man favorite actress kirsten dunst rocks anyway never ceases amaze richard kiel wow he's definitely got part jaw's nailed i've seen movie's he's awesome matter fact grandparent's met mr kiel jealous told kirsten dunst top list celebritie's want meet john cleese breathtaking never seen better person play wisecracking gadget creating q mr cleese hilarious i've seen work pierce brosnan goldeneye tommorow never dies he's awesome john cleese's recent project shrek starring mike myer's cameron diaz julie andrew's eddie murphy shrek theatre's good luck oh yeah q alway's says grow\n"
     ]
    }
   ],
   "source": [
    "print(train_set.loc[0,\"reseña_limpia_sin_stopwords\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sobre tokens y tokenizacion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjate en como ha quedado convertido el texto original de cada reseña. Lo hemos reducido a **palabras significativas o \"tokens\"**. En el contexto de NLP, un token es la unidad básica de procesamiento de textos, y, normalmente, se refiere a palabras con significado. Pero con la evolución de la IA y del NLP, el token ha pasado de tener la obligación de ese significado semántico, ahora hay subtokens (como las terminaciones o los prefijos) y una definición más adecuada de token sería: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Token** = Unidad básica de texto procesada. Puede ser una palabra, un carácter, o incluso un subconjunto de una palabra. Los tokens son los componentes individuales en los que se divide el texto para su análisis o para entrenar modelos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, vamos a seguir considerando tokens a las palabras, pero es iportante que tengas en cuenta que ya no siemprbe coinciden con palabras enteras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **tokenizacion** es el proceso por el cual un texto se convierte en tokens. Lo hemos hecho a mano, para que veas el proceso, pero existen tokenizadores ya especializados (por ejemplo nltk tiene sus tokenizer, prueba a jugar con ellos).\n",
    "\n",
    "Una vez tokenizados, convertidos a tokens, la vectorización es la que nos permite ahora convertir esos tokens en números y con ello en features asignables a cada instancia. Vamos a ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.rename(columns = {\"reseña_limpia_sin_stopwords\": \"tokenizada\"}, inplace = True)\n",
    "test_set.rename(columns = {\"reseña_limpia_sin_stopwords\": \"tokenizada\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorización de Texto (I): Bag_of_words (BoW)\n",
    "\n",
    "\n",
    "Vamos a contruir un vector con tantas dimensiones como palabras/tokens haya (en todo el dataset) y para cada texto y cada dimensión vamos a rellenar con el número de veces que aparece el token en cada texto o bien binariamente (1 si aparece 1 o más veces, 0 si no aparece) o bien frecuencialmente (pondremos el número de veces que aparece el token en el texto).\n",
    "* Al conjunto de tokens de todo un dataset se le llama vocabulary\n",
    "* Al conjunto de todos los textos también se le llama corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizacion binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a apoyarnos en sklearn y en concreto en `CountVectorizer`. Esta clase, al hacer un `fit` construye un diccionario de tokens con todos los tokens diferentes que encuentra en el dataset que se le pasa como parámetro al método `fit`. Se puede limitar dicho diccionario de diversas formas (eliminando palabras muy frecuentes y poco frecuentes, quedándote sólo con las que superan una frecuencia de aparición considerando todas las instancias, etc) y además se puede pasar un diccionario como argumento. Te invito a que investigues todas sus posibilidades, y no sólo las que tienen que ver con el diccionario, [aquí](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True, max_features=32000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True, max_features=32000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(binary=True, max_features=32000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Si aparece una palabra en una review, le pone un 1. Da igual que aparezca 100 veces, no cuenta. Xq binary=True\n",
    "# Solo pone 1s cuando detecta una palabra en una review\n",
    "prepro_vectorizer = CountVectorizer(binary=True, max_features = 32000) # Vamos a considerar un diccionario con los 32k tokens más usados\n",
    "prepro_vectorizer.fit(train_set[\"tokenizada\"]) # Al hacer el fit, se construye un diccionario de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'always': 809,\n",
       " 'huge': 13601,\n",
       " 'james': 15032,\n",
       " 'bond': 3046,\n",
       " 'fanatic': 10106,\n",
       " 'seen': 25009,\n",
       " 'almost': 759,\n",
       " 'films': 10486,\n",
       " 'except': 9701,\n",
       " 'die': 7592,\n",
       " 'another': 1090,\n",
       " 'day': 6871,\n",
       " 'world': 31611,\n",
       " 'enough': 9277,\n",
       " 'graphic': 12129,\n",
       " 'everything': 9631,\n",
       " 'nothing': 19390,\n",
       " 'breathtaking': 3347,\n",
       " 'voice': 30762,\n",
       " 'talents': 28094,\n",
       " 'wow': 31649,\n",
       " 'love': 16785,\n",
       " 'pierce': 21059,\n",
       " 'brosnan': 3496,\n",
       " 'finally': 10504,\n",
       " 'video': 30581,\n",
       " 'game': 11418,\n",
       " 'enjoyed': 9254,\n",
       " 'past': 20502,\n",
       " 'games': 11424,\n",
       " 'goldeneye': 11929,\n",
       " 'agent': 509,\n",
       " 'fire': 10546,\n",
       " 'one': 19745,\n",
       " 'definitely': 7087,\n",
       " 'best': 2548,\n",
       " 'mr': 18666,\n",
       " 'may': 17579,\n",
       " 'call': 3860,\n",
       " 'sign': 25680,\n",
       " 'respect': 23536,\n",
       " 'yes': 31821,\n",
       " 'phenomenally': 20951,\n",
       " 'exciting': 9725,\n",
       " 'hear': 12908,\n",
       " 'long': 16687,\n",
       " 'last': 16053,\n",
       " 'duh': 8560,\n",
       " 've': 30431,\n",
       " 'perform': 20783,\n",
       " 'robin': 23970,\n",
       " 'williams': 31385,\n",
       " 'let': 16312,\n",
       " 'tell': 28320,\n",
       " 'make': 17124,\n",
       " 'great': 12173,\n",
       " 'team': 28238,\n",
       " 'funny': 11313,\n",
       " 'wickedly': 31329,\n",
       " 'handsome': 12649,\n",
       " 'mean': 17675,\n",
       " 'say': 24605,\n",
       " 'good': 11952,\n",
       " 'way': 31069,\n",
       " 'actor': 248,\n",
       " 'would': 31639,\n",
       " 'want': 30935,\n",
       " 'walk': 30881,\n",
       " 'wrap': 31657,\n",
       " 'arms': 1383,\n",
       " 'around': 1391,\n",
       " 'hug': 13600,\n",
       " 'saying': 24606,\n",
       " 'thank': 28480,\n",
       " 'know': 15755,\n",
       " 'he': 12870,\n",
       " 'fan': 10105,\n",
       " 'willem': 31381,\n",
       " 'dafoe': 6697,\n",
       " 'even': 9608,\n",
       " 'though': 28621,\n",
       " 'couple': 6157,\n",
       " 'movies': 18634,\n",
       " 'role': 24027,\n",
       " 'nikolai': 19233,\n",
       " 'brilliant': 3423,\n",
       " 'spell': 26584,\n",
       " 'character': 4436,\n",
       " 'name': 18882,\n",
       " 'right': 23841,\n",
       " 'lol': 16669,\n",
       " 'job': 15209,\n",
       " 'accent': 116,\n",
       " 'sometimes': 26364,\n",
       " 'can': 3918,\n",
       " 'two': 29561,\n",
       " 'finding': 10517,\n",
       " 'nemo': 19089,\n",
       " 'spider': 26608,\n",
       " 'man': 17190,\n",
       " 'favorite': 10234,\n",
       " 'actress': 252,\n",
       " 'kirsten': 15682,\n",
       " 'dunst': 8611,\n",
       " 'rocks': 23998,\n",
       " 'anyway': 1161,\n",
       " 'never': 19134,\n",
       " 'ceases': 4283,\n",
       " 'amaze': 826,\n",
       " 'richard': 23787,\n",
       " 'kiel': 15624,\n",
       " 'got': 12019,\n",
       " 'part': 20437,\n",
       " 'jaw': 15082,\n",
       " 'nailed': 18864,\n",
       " 'movie': 18617,\n",
       " 'awesome': 1843,\n",
       " 'matter': 17546,\n",
       " 'fact': 10016,\n",
       " 'grandparent': 12114,\n",
       " 'met': 17892,\n",
       " 'jealous': 15096,\n",
       " 'told': 28876,\n",
       " 'top': 28938,\n",
       " 'list': 16534,\n",
       " 'meet': 17733,\n",
       " 'john': 15232,\n",
       " 'cleese': 4955,\n",
       " 'better': 2572,\n",
       " 'person': 20859,\n",
       " 'play': 21237,\n",
       " 'wisecracking': 31467,\n",
       " 'gadget': 11367,\n",
       " 'creating': 6306,\n",
       " 'hilarious': 13198,\n",
       " 'work': 31596,\n",
       " 'dies': 7598,\n",
       " 'recent': 22887,\n",
       " 'project': 22030,\n",
       " 'shrek': 25580,\n",
       " 'starring': 26952,\n",
       " 'mike': 17989,\n",
       " 'myer': 18814,\n",
       " 'cameron': 3894,\n",
       " 'diaz': 7560,\n",
       " 'julie': 15364,\n",
       " 'andrew': 980,\n",
       " 'eddie': 8783,\n",
       " 'murphy': 18754,\n",
       " 'theatre': 28503,\n",
       " 'luck': 16852,\n",
       " 'oh': 19679,\n",
       " 'yeah': 31792,\n",
       " 'alway': 808,\n",
       " 'says': 24610,\n",
       " 'grow': 12328,\n",
       " 'christian': 4730,\n",
       " 'terrible': 28414,\n",
       " 'acting': 233,\n",
       " 'unreal': 30088,\n",
       " 'situations': 25819,\n",
       " 'completely': 5462,\n",
       " 'facade': 10000,\n",
       " 'front': 11229,\n",
       " 'christianity': 4731,\n",
       " 'might': 17980,\n",
       " 'well': 31174,\n",
       " 'watch': 31022,\n",
       " 'remember': 23277,\n",
       " 'titans': 28824,\n",
       " 'least': 16188,\n",
       " 'mix': 18240,\n",
       " 'christ': 4726,\n",
       " 'football': 10891,\n",
       " 'film': 10464,\n",
       " 'like': 16441,\n",
       " 'formulaic': 10989,\n",
       " 'steroid': 27083,\n",
       " 'losers': 16748,\n",
       " 'really': 22820,\n",
       " 'pressing': 21810,\n",
       " 'comments': 5362,\n",
       " 'bothered': 3165,\n",
       " 'school': 24733,\n",
       " 'georgia': 11635,\n",
       " 'white': 31286,\n",
       " 'academy': 113,\n",
       " 'notice': 19392,\n",
       " 'single': 25774,\n",
       " 'black': 2739,\n",
       " 'student': 27376,\n",
       " 'player': 21242,\n",
       " 'deal': 6902,\n",
       " 'south': 26456,\n",
       " 'built': 3606,\n",
       " 'reason': 22838,\n",
       " 'segregation': 25024,\n",
       " 'troubling': 29382,\n",
       " 'changing': 4415,\n",
       " 'hearts': 12934,\n",
       " 'note': 19384,\n",
       " 'loved': 16791,\n",
       " 'token': 28873,\n",
       " 'coach': 5098,\n",
       " 'made': 17029,\n",
       " 'entirely': 9337,\n",
       " 'giants': 11701,\n",
       " 'players': 21243,\n",
       " 'uncritical': 29744,\n",
       " 'acclaim': 138,\n",
       " 'everyone': 9629,\n",
       " 'get': 11665,\n",
       " 'people': 20743,\n",
       " 'bad': 1934,\n",
       " 'filming': 10478,\n",
       " 'writing': 31700,\n",
       " 'short': 25512,\n",
       " 'poor': 21469,\n",
       " 'compared': 5412,\n",
       " 'compare': 5411,\n",
       " 'charisma': 4463,\n",
       " 'energy': 9207,\n",
       " 'half': 12566,\n",
       " 'baked': 1979,\n",
       " 'shown': 25570,\n",
       " 'less': 16303,\n",
       " 'sense': 25094,\n",
       " 'unrealistic': 30089,\n",
       " 'ending': 9183,\n",
       " 'simply': 25748,\n",
       " 'reading': 22795,\n",
       " 'little': 16561,\n",
       " 'scripture': 24873,\n",
       " 'praying': 21668,\n",
       " 'begin': 2371,\n",
       " 'complete': 5460,\n",
       " 'life': 16399,\n",
       " 'change': 4410,\n",
       " 'wonder': 31553,\n",
       " 'listen': 16536,\n",
       " 'christians': 4732,\n",
       " 'sell': 25049,\n",
       " 'cure': 6577,\n",
       " 'need': 19034,\n",
       " 'sold': 26311,\n",
       " 'attached': 1641,\n",
       " 'whimsical': 31254,\n",
       " 'lifestyles': 16413,\n",
       " 'considers': 5754,\n",
       " 'anti': 1112,\n",
       " 'intellectual': 14587,\n",
       " 'presentation': 21785,\n",
       " 'recognize': 22921,\n",
       " 'inspirational': 14514,\n",
       " 'give': 11774,\n",
       " 'real': 22802,\n",
       " 'picture': 21045,\n",
       " 'struggle': 27360,\n",
       " 'heartfelt': 12928,\n",
       " 'tension': 28383,\n",
       " 'importantly': 14038,\n",
       " 'means': 17687,\n",
       " 'stretch': 27298,\n",
       " 'values': 30364,\n",
       " 'falls': 10084,\n",
       " 'pull': 22273,\n",
       " 'message': 17882,\n",
       " 'shallow': 25294,\n",
       " 'positive': 21545,\n",
       " 'thinking': 28582,\n",
       " 'live': 16566,\n",
       " 'alongside': 763,\n",
       " 'third': 28587,\n",
       " 'living': 16582,\n",
       " 'conditions': 5607,\n",
       " 'hollow': 13350,\n",
       " 'churches': 4774,\n",
       " 'today': 28856,\n",
       " 'religion': 23235,\n",
       " 'neatly': 19017,\n",
       " 'sandwiched': 24485,\n",
       " 'stranger': 27244,\n",
       " 'small': 26084,\n",
       " 'noir': 19295,\n",
       " 'proved': 22167,\n",
       " 'welles': 31177,\n",
       " 'formidable': 10985,\n",
       " 'genre': 11604,\n",
       " 'budget': 3570,\n",
       " 'time': 28761,\n",
       " 'ironically': 14867,\n",
       " 'biggest': 2637,\n",
       " 'box': 3219,\n",
       " 'office': 19657,\n",
       " 'success': 27534,\n",
       " 'forties': 11004,\n",
       " 'macbeth': 16978,\n",
       " 'shakespeare': 25286,\n",
       " 'adaptation': 272,\n",
       " 'shot': 25528,\n",
       " 'old': 19696,\n",
       " 'western': 31203,\n",
       " 'days': 6877,\n",
       " 'comes': 5316,\n",
       " 'lady': 15921,\n",
       " 'dark': 6803,\n",
       " 'woven': 31648,\n",
       " 'fabric': 9992,\n",
       " 'mythos': 18843,\n",
       " 'covers': 6192,\n",
       " 'magnificent': 17076,\n",
       " 'ambersons': 838,\n",
       " 'director': 7706,\n",
       " 'manage': 17192,\n",
       " 'save': 24583,\n",
       " 'clutches': 5090,\n",
       " 'studio': 27381,\n",
       " 'bosses': 3154,\n",
       " 'six': 25822,\n",
       " 'years': 31801,\n",
       " 'maltese': 17176,\n",
       " 'falcon': 10073,\n",
       " 'post': 21562,\n",
       " 'war': 30941,\n",
       " 'craze': 6289,\n",
       " 'full': 11281,\n",
       " 'swing': 27910,\n",
       " 'ahead': 547,\n",
       " 'true': 29399,\n",
       " 'visionary': 30712,\n",
       " 'tremendous': 29270,\n",
       " 'artistic': 1448,\n",
       " 'integrity': 14585,\n",
       " 'envisioned': 9374,\n",
       " 'labyrinthine': 15894,\n",
       " 'shadows': 25267,\n",
       " 'already': 774,\n",
       " 'darker': 6806,\n",
       " 'sinister': 25783,\n",
       " 'paranoid': 20389,\n",
       " 'serpentine': 25171,\n",
       " 'anything': 1159,\n",
       " 'contemporaries': 5831,\n",
       " 'misunderstood': 18230,\n",
       " 'point': 21361,\n",
       " 'hour': 13557,\n",
       " 'footage': 10889,\n",
       " 'forever': 10952,\n",
       " 'left': 16220,\n",
       " 'cutting': 6651,\n",
       " 'room': 24082,\n",
       " 'floor': 10764,\n",
       " 'europe': 9577,\n",
       " 'championed': 4391,\n",
       " 'classic': 4902,\n",
       " 'certain': 4347,\n",
       " 'set': 25192,\n",
       " 'pieces': 21055,\n",
       " 'stand': 26907,\n",
       " 'aquarium': 1274,\n",
       " 'scene': 24677,\n",
       " 'flickering': 10716,\n",
       " 'light': 16424,\n",
       " 'ominous': 19727,\n",
       " 'course': 6166,\n",
       " 'funhouse': 11306,\n",
       " 'hall': 12569,\n",
       " 'mirrors': 18139,\n",
       " 'finale': 10501,\n",
       " 'piece': 21053,\n",
       " 'bravura': 3306,\n",
       " 'citizen': 4841,\n",
       " 'kane': 15445,\n",
       " 'trial': 29288,\n",
       " 'faults': 10221,\n",
       " 'find': 10515,\n",
       " 'ill': 13871,\n",
       " 'advised': 416,\n",
       " 'irish': 14857,\n",
       " 'perhaps': 20794,\n",
       " 'erratic': 9456,\n",
       " 'editing': 8803,\n",
       " 'first': 10566,\n",
       " 'act': 231,\n",
       " 'story': 27210,\n",
       " 'however': 13580,\n",
       " 'progressively': 22027,\n",
       " 'mesmerizing': 17880,\n",
       " 'manner': 17261,\n",
       " 'cuts': 6648,\n",
       " 'serve': 25178,\n",
       " 'intensify': 14606,\n",
       " 'believe': 2423,\n",
       " 'heavily': 12954,\n",
       " 'chopped': 4703,\n",
       " 'versions': 30524,\n",
       " 'attain': 1652,\n",
       " 'surreal': 27772,\n",
       " 'quality': 22412,\n",
       " 'exile': 9789,\n",
       " 'ten': 28356,\n",
       " 'return': 23647,\n",
       " 'deliver': 7160,\n",
       " 'yet': 31824,\n",
       " 'stonewall': 27179,\n",
       " 'monumental': 18434,\n",
       " 'touch': 29011,\n",
       " 'evil': 9641,\n",
       " 'crowning': 6462,\n",
       " 'jewel': 15164,\n",
       " 'waning': 30930,\n",
       " 'star': 26927,\n",
       " 'vehicle': 30443,\n",
       " 'hollywood': 13354,\n",
       " 'premiere': 21747,\n",
       " 'rita': 23904,\n",
       " 'hayworth': 12855,\n",
       " 'ought': 19968,\n",
       " 'marriage': 17396,\n",
       " 'ended': 9178,\n",
       " 'released': 23214,\n",
       " 'sixty': 25827,\n",
       " 'later': 16066,\n",
       " 'still': 27115,\n",
       " 'pictures': 21047,\n",
       " 'likely': 16446,\n",
       " 'discover': 7796,\n",
       " 'surely': 27737,\n",
       " 'must': 18785,\n",
       " 'count': 6134,\n",
       " 'something': 26360,\n",
       " 'ago': 529,\n",
       " 'follow': 10859,\n",
       " 'soap': 26245,\n",
       " 'tv': 29519,\n",
       " 'curious': 6584,\n",
       " 'rewarded': 23734,\n",
       " 'marvelous': 17435,\n",
       " 'spoof': 26701,\n",
       " 'soaps': 26248,\n",
       " 'jealousies': 15097,\n",
       " 'usual': 30294,\n",
       " 'actors': 249,\n",
       " 'insecurities': 14476,\n",
       " 'sorts': 26427,\n",
       " 'lovely': 16798,\n",
       " 'excesses': 9711,\n",
       " 'amazing': 830,\n",
       " 'cast': 4176,\n",
       " 'incredible': 14194,\n",
       " 'script': 24869,\n",
       " 'someone': 26352,\n",
       " 'notch': 19382,\n",
       " 'silly': 25716,\n",
       " 'sort': 26421,\n",
       " 'writers': 31696,\n",
       " 'write': 31694,\n",
       " 'perfect': 20778,\n",
       " 'lines': 16498,\n",
       " 'anyone': 1158,\n",
       " 'talk': 28099,\n",
       " 'admit': 341,\n",
       " 'hearing': 12910,\n",
       " 'highly': 13180,\n",
       " 'recommend': 22931,\n",
       " 'seeing': 24995,\n",
       " 'll': 16591,\n",
       " 'throw': 28670,\n",
       " 'end': 9166,\n",
       " 'sitting': 25814,\n",
       " 'couch': 6114,\n",
       " 'watching': 31030,\n",
       " 'laughing': 16084,\n",
       " 'thoroughly': 28619,\n",
       " 'enjoying': 9255,\n",
       " 'whole': 31301,\n",
       " 'wonderful': 31556,\n",
       " 'thing': 28576,\n",
       " 'many': 17293,\n",
       " 'congrats': 5675,\n",
       " 'here': 13078,\n",
       " 'gritty': 12280,\n",
       " 'guys': 12472,\n",
       " 'revenge': 23675,\n",
       " 'relentless': 23218,\n",
       " 'rough': 24147,\n",
       " 'denzel': 7267,\n",
       " 'washington': 31002,\n",
       " 'three': 28646,\n",
       " 'personalities': 20865,\n",
       " 'low': 16809,\n",
       " 'key': 15581,\n",
       " 'drunk': 8506,\n",
       " 'former': 10983,\n",
       " 'mercenary': 17836,\n",
       " 'loving': 16806,\n",
       " 'father': 10204,\n",
       " 'type': 29575,\n",
       " 'girl': 11756,\n",
       " 'brutal': 3531,\n",
       " 'maniac': 17232,\n",
       " 'loose': 16720,\n",
       " 'seeking': 25000,\n",
       " 'answers': 1096,\n",
       " 'hired': 13250,\n",
       " 'bodyguard': 2981,\n",
       " 'american': 864,\n",
       " 'mexico': 17929,\n",
       " 'kidnappings': 15619,\n",
       " 'children': 4639,\n",
       " 'occur': 19599,\n",
       " 'regularly': 23134,\n",
       " 'according': 160,\n",
       " 'becomes': 2322,\n",
       " 'kid': 15606,\n",
       " 'played': 21241,\n",
       " 'winningly': 31444,\n",
       " 'child': 4633,\n",
       " 'dakota': 6713,\n",
       " 'fanning': 10124,\n",
       " 'kidnapped': 15615,\n",
       " 'goes': 11912,\n",
       " 'men': 17801,\n",
       " 'responsible': 23557,\n",
       " 'spares': 26501,\n",
       " 'beware': 2584,\n",
       " 'squeamish': 26807,\n",
       " 'stylish': 27431,\n",
       " 'making': 17134,\n",
       " 'liked': 16444,\n",
       " 'number': 19466,\n",
       " 'found': 11031,\n",
       " 'frenetic': 11156,\n",
       " 'tastes': 28190,\n",
       " 'camera': 3889,\n",
       " 'could': 6121,\n",
       " 'headache': 12872,\n",
       " 'thought': 28622,\n",
       " 'fit': 10586,\n",
       " 'tense': 28381,\n",
       " 'storyline': 27214,\n",
       " 'fascinating': 10175,\n",
       " 'view': 30599,\n",
       " 'shaky': 25291,\n",
       " 'besides': 2541,\n",
       " 'stars': 26954,\n",
       " 'interesting': 14636,\n",
       " 'christopher': 4740,\n",
       " 'walken': 30884,\n",
       " 'uncharacteristically': 29706,\n",
       " 'fine': 10520,\n",
       " 'panders': 20308,\n",
       " 'base': 2178,\n",
       " 'emotions': 9092,\n",
       " 'us': 30273,\n",
       " 'works': 31609,\n",
       " 'baseball': 2179,\n",
       " 'since': 25760,\n",
       " 'redford': 23002,\n",
       " 'whacked': 31212,\n",
       " 'natural': 18970,\n",
       " 'dennis': 7246,\n",
       " 'quaid': 22403,\n",
       " 'rachel': 22503,\n",
       " 'griffiths': 12241,\n",
       " 'screen': 24845,\n",
       " 'seemed': 25004,\n",
       " 'lives': 16576,\n",
       " 'laced': 15898,\n",
       " 'dreams': 8408,\n",
       " 'dripping': 8453,\n",
       " 'reality': 22813,\n",
       " 'dream': 8398,\n",
       " 'devil': 7496,\n",
       " 'ray': 22766,\n",
       " 'mid': 17960,\n",
       " 'rookie': 24080,\n",
       " 'jimmy': 15189,\n",
       " 'morris': 18513,\n",
       " 'australian': 1740,\n",
       " 'born': 3136,\n",
       " 'plays': 21252,\n",
       " 'native': 18966,\n",
       " 'west': 31201,\n",
       " 'texan': 28462,\n",
       " 'lot': 16755,\n",
       " 'texans': 28463,\n",
       " 'perfection': 20780,\n",
       " 'wannabe': 30932,\n",
       " 'humble': 13636,\n",
       " 'winner': 31438,\n",
       " 'much': 18676,\n",
       " 'psychological': 22226,\n",
       " 'baggage': 1959,\n",
       " 'average': 1796,\n",
       " 'viewer': 30602,\n",
       " 'audience': 1706,\n",
       " 'chemistry': 4581,\n",
       " 'heart': 12918,\n",
       " 'going': 11919,\n",
       " 'ingredients': 14393,\n",
       " 'americana': 865,\n",
       " 'apple': 1227,\n",
       " 'pie': 21052,\n",
       " 'popcorn': 21475,\n",
       " 'became': 2311,\n",
       " 'la': 15873,\n",
       " 'mode': 18278,\n",
       " 'hey': 13134,\n",
       " 'buy': 3767,\n",
       " 'cd': 4278,\n",
       " 'music': 18771,\n",
       " 'carries': 4115,\n",
       " 'magnificently': 17077,\n",
       " 'syncing': 27991,\n",
       " 'words': 31593,\n",
       " 'pushes': 22365,\n",
       " 'forward': 11018,\n",
       " 'exactly': 9669,\n",
       " 'area': 1324,\n",
       " 'disappoints': 7743,\n",
       " 'often': 19673,\n",
       " 'criticisms': 6396,\n",
       " 'given': 11776,\n",
       " 'somebody': 26349,\n",
       " 'else': 9004,\n",
       " 'teach': 28230,\n",
       " 'st': 26831,\n",
       " 'nuns': 19475,\n",
       " 'nice': 19173,\n",
       " 'decoration': 7009,\n",
       " 'place': 21179,\n",
       " 'open': 19784,\n",
       " 'close': 5038,\n",
       " 'worth': 31635,\n",
       " 'every': 9625,\n",
       " 'minute': 18117,\n",
       " 'analysis': 943,\n",
       " 'greatest': 12175,\n",
       " 'enthusiasm': 9328,\n",
       " 'advance': 387,\n",
       " 'screening': 24848,\n",
       " 'oldest': 19698,\n",
       " 'complex': 5471,\n",
       " 'tales': 28095,\n",
       " 'known': 15763,\n",
       " 'mankind': 17255,\n",
       " 'epic': 9386,\n",
       " 'read': 22791,\n",
       " 'tolkien': 28882,\n",
       " 'disappointment': 7741,\n",
       " 'plot': 21293,\n",
       " 'joke': 15248,\n",
       " 'turned': 29503,\n",
       " 'opera': 19792,\n",
       " 'elements': 8941,\n",
       " 'faithful': 10064,\n",
       " 'sprinkled': 26758,\n",
       " 'throughout': 28669,\n",
       " 'haphazard': 12678,\n",
       " 'times': 28774,\n",
       " 'paid': 20233,\n",
       " 'lip': 16523,\n",
       " 'service': 25181,\n",
       " 'battle': 2238,\n",
       " 'battles': 2241,\n",
       " 'see': 24989,\n",
       " 'achilles': 195,\n",
       " 'strange': 27240,\n",
       " 'combination': 5291,\n",
       " 'nearly': 19014,\n",
       " 'matrix': 17540,\n",
       " 'powers': 21628,\n",
       " 'utter': 30309,\n",
       " 'ruthlessness': 24298,\n",
       " 'male': 17149,\n",
       " 'lovers': 16801,\n",
       " 'original': 19910,\n",
       " 'poem': 21349,\n",
       " 'turn': 29501,\n",
       " 'fabio': 9988,\n",
       " 'beach': 2264,\n",
       " 'guise': 12402,\n",
       " 'pitt': 21166,\n",
       " 'effort': 8848,\n",
       " 'warrior': 30987,\n",
       " 'figure': 10446,\n",
       " 'ever': 9617,\n",
       " 'produced': 21972,\n",
       " 'actually': 257,\n",
       " 'decent': 6970,\n",
       " 'trying': 29430,\n",
       " 'ridiculous': 23821,\n",
       " 'waste': 31010,\n",
       " 'talent': 28091,\n",
       " 'peter': 20914,\n",
       " 'toole': 28926,\n",
       " 'stole': 27162,\n",
       " 'show': 25548,\n",
       " 'orlando': 19919,\n",
       " 'bloom': 2900,\n",
       " 'sean': 24924,\n",
       " 'bean': 2275,\n",
       " 'pathetic': 20533,\n",
       " 'lotr': 16756,\n",
       " 'gladiator': 11786,\n",
       " 'hold': 13333,\n",
       " 'candle': 3937,\n",
       " 'plenty': 21281,\n",
       " 'hunks': 13683,\n",
       " 'ladies': 15919,\n",
       " 'goggle': 11914,\n",
       " 'scenes': 24680,\n",
       " 'siege': 25663,\n",
       " 'pay': 20601,\n",
       " 'dragged': 8346,\n",
       " 'dirt': 7712,\n",
       " 'characters': 4450,\n",
       " 'cent': 4325,\n",
       " 'deals': 6907,\n",
       " 'yore': 31847,\n",
       " 'far': 10136,\n",
       " 'realize': 22816,\n",
       " 'color': 5261,\n",
       " 'effects': 8840,\n",
       " 'extraordinary': 9949,\n",
       " 'vs': 30818,\n",
       " 'crap': 6260,\n",
       " 'earth': 8719,\n",
       " 'flying': 10824,\n",
       " 'saucers': 24568,\n",
       " 'killer': 15633,\n",
       " 'universality': 29996,\n",
       " 'hinges': 13233,\n",
       " 'principles': 21903,\n",
       " 'ancient': 965,\n",
       " 'concept': 5556,\n",
       " 'hidden': 13157,\n",
       " 'incestuous': 14133,\n",
       " 'desire': 7385,\n",
       " 'daughter': 6847,\n",
       " 'idea': 13798,\n",
       " 'happen': 12681,\n",
       " 'tech': 28259,\n",
       " 'reach': 22779,\n",
       " 'desires': 7388,\n",
       " 'become': 2321,\n",
       " 'manifest': 17239,\n",
       " 'needless': 19040,\n",
       " 'wished': 31474,\n",
       " 'dead': 6892,\n",
       " 'gets': 11667,\n",
       " 'fate': 10200,\n",
       " 'forbidden': 10910,\n",
       " 'planet': 21201,\n",
       " 'populace': 21486,\n",
       " 'rather': 22727,\n",
       " 'modern': 18288,\n",
       " 'unfortunately': 29922,\n",
       " 'gear': 11534,\n",
       " 'the': 28500,\n",
       " 'krell': 15820,\n",
       " 'wasted': 31011,\n",
       " 'ignorance': 13853,\n",
       " 'remains': 23261,\n",
       " 'morpheus': 18508,\n",
       " 'knows': 15764,\n",
       " 'access': 129,\n",
       " 'prevent': 21853,\n",
       " 'losing': 16751,\n",
       " 'beautifully': 2305,\n",
       " 'rendered': 23313,\n",
       " 'considering': 5753,\n",
       " 'age': 502,\n",
       " 'stunning': 27401,\n",
       " 'suspense': 27808,\n",
       " 'action': 235,\n",
       " 'human': 13621,\n",
       " 'emotion': 9087,\n",
       " 'visually': 30727,\n",
       " 'pass': 20477,\n",
       " 'sci': 24762,\n",
       " 'fi': 10404,\n",
       " 'content': 5840,\n",
       " 'elevates': 8948,\n",
       " 'beyond': 2592,\n",
       " 'timeless': 28768,\n",
       " 'pure': 22330,\n",
       " 'synth': 28000,\n",
       " 'perfectly': 20782,\n",
       " 'background': 1912,\n",
       " 'not': 19378,\n",
       " 'miss': 18196,\n",
       " 'headed': 12874,\n",
       " 'comedy': 5311,\n",
       " 'family': 10099,\n",
       " 'son': 26369,\n",
       " 'called': 3864,\n",
       " 'pecker': 20648,\n",
       " 'use': 30277,\n",
       " 'peck': 20646,\n",
       " 'food': 10879,\n",
       " 'loves': 16802,\n",
       " 'take': 28078,\n",
       " 'kinds': 15658,\n",
       " 'suburb': 27519,\n",
       " 'baltimore': 2028,\n",
       " 'md': 17667,\n",
       " 'manages': 17198,\n",
       " 'attention': 1669,\n",
       " 'group': 12323,\n",
       " 'photo': 20993,\n",
       " 'art': 1422,\n",
       " 'new': 19139,\n",
       " 'york': 31848,\n",
       " 'city': 4845,\n",
       " 'cute': 6637,\n",
       " 'sister': 25803,\n",
       " 'nuts': 19489,\n",
       " 'sugar': 27580,\n",
       " 'addict': 283,\n",
       " 'taking': 28085,\n",
       " 'bag': 1957,\n",
       " 'showing': 25565,\n",
       " 'lumps': 16897,\n",
       " 'jockey': 15215,\n",
       " 'grinding': 12260,\n",
       " 'movements': 18614,\n",
       " 'gals': 11412,\n",
       " 'pretty': 21847,\n",
       " 'hard': 12705,\n",
       " 'keep': 15528,\n",
       " 'mind': 18058,\n",
       " 'gutter': 12467,\n",
       " 'cares': 4051,\n",
       " 'laughs': 16086,\n",
       " 'simple': 25741,\n",
       " 'expecting': 9827,\n",
       " 'mainland': 17107,\n",
       " 'china': 4660,\n",
       " 'prepared': 21762,\n",
       " 'dynamics': 8679,\n",
       " 'community': 5399,\n",
       " 'inevitability': 14291,\n",
       " 'rarely': 22705,\n",
       " 'explored': 9880,\n",
       " 'expertly': 9849,\n",
       " 'solid': 26321,\n",
       " 'drawn': 8390,\n",
       " 'organization': 19889,\n",
       " 'audiences': 1707,\n",
       " 'accustomed': 183,\n",
       " 'difficulty': 7617,\n",
       " 'following': 10863,\n",
       " 'progression': 22025,\n",
       " 'subtitles': 27510,\n",
       " 'jiang': 15176,\n",
       " 'wu': 31716,\n",
       " 'retarded': 23613,\n",
       " 'brother': 3499,\n",
       " 'constant': 5776,\n",
       " 'shining': 25443,\n",
       " 'leave': 16194,\n",
       " 'cynicism': 6673,\n",
       " 'locker': 16638,\n",
       " 'check': 4534,\n",
       " 'realised': 22805,\n",
       " 'lost': 16754,\n",
       " 'valuable': 30361,\n",
       " 'precious': 21687,\n",
       " 'moments': 18350,\n",
       " 'back': 1903,\n",
       " 'dire': 7696,\n",
       " 'reminded': 23284,\n",
       " 'stereotypical': 27076,\n",
       " 'sitcom': 25807,\n",
       " 'regret': 23123,\n",
       " 'race': 22497,\n",
       " 'issue': 14936,\n",
       " 'apparently': 1203,\n",
       " 'mate': 17510,\n",
       " 'think': 28579,\n",
       " 'paint': 20244,\n",
       " 'dry': 8512,\n",
       " 'entertaining': 9319,\n",
       " 'funnier': 11310,\n",
       " 'pile': 21075,\n",
       " 'drivel': 8459,\n",
       " 'please': 21267,\n",
       " 'stop': 27191,\n",
       " 'probably': 21939,\n",
       " 'entire': 9336,\n",
       " 'dialog': 7542,\n",
       " 'backwards': 1929,\n",
       " 'hungary': 13678,\n",
       " 'young': 31858,\n",
       " 'stewart': 27096,\n",
       " 'eligible': 8963,\n",
       " 'bachelor': 1898,\n",
       " 'kralik': 15817,\n",
       " 'secret': 24958,\n",
       " 'admirer': 336,\n",
       " 'margaret': 17321,\n",
       " 'sullavan': 27610,\n",
       " 'innocent': 14445,\n",
       " 'klara': 15706,\n",
       " 'secretly': 24963,\n",
       " 'pen': 20701,\n",
       " 'friend': 11190,\n",
       " 'together': 28866,\n",
       " 'confides': 5639,\n",
       " 'letters': 16322,\n",
       " 'clearly': 4950,\n",
       " 'besotted': 2544,\n",
       " 'unable': 29647,\n",
       " 'feelings': 10300,\n",
       " 'whilst': 31249,\n",
       " 'competition': 5440,\n",
       " 'confused': 5666,\n",
       " 'wont': 31566,\n",
       " 'sweet': 27882,\n",
       " 'sugary': 27581,\n",
       " 'mentioning': 17825,\n",
       " 'frank': 11089,\n",
       " 'morgan': 18482,\n",
       " 'playing': 21249,\n",
       " 'shop': 25503,\n",
       " 'owner': 20180,\n",
       " 'hugo': 13608,\n",
       " 'matuschek': 17558,\n",
       " 'felix': 10311,\n",
       " 'bressart': 3369,\n",
       " 'pirovitch': 21144,\n",
       " 'confidant': 5632,\n",
       " 'joseph': 15283,\n",
       " 'schildkraut': 24703,\n",
       " 'womanising': 31545,\n",
       " 'arrogant': 1414,\n",
       " 'vadas': 30324,\n",
       " 'cannot': 3958,\n",
       " 'help': 13027,\n",
       " 'hate': 12805,\n",
       " 'beginning': 2374,\n",
       " 'william': 31384,\n",
       " 'tracy': 29077,\n",
       " 'endear': 9170,\n",
       " 'confident': 5636,\n",
       " 'upstart': 30243,\n",
       " 'junior': 15388,\n",
       " 'pepi': 20748,\n",
       " 'recently': 22888,\n",
       " 'mail': 17099,\n",
       " 'tom': 28886,\n",
       " 'hanks': 12667,\n",
       " 'meg': 17737,\n",
       " 'ryan': 24306,\n",
       " 'although': 798,\n",
       " 'suspect': 27801,\n",
       " 'younger': 31859,\n",
       " 'disagree': 7725,\n",
       " 'christmas': 4736,\n",
       " 'suggest': 27582,\n",
       " 'pour': 21608,\n",
       " 'glass': 11808,\n",
       " 'wine': 31426,\n",
       " 'put': 22373,\n",
       " 'log': 16653,\n",
       " 'kleenex': 15710,\n",
       " 'handy': 12652,\n",
       " 'waqt': 30940,\n",
       " 'example': 9684,\n",
       " 'chicken': 4621,\n",
       " 'soup': 26447,\n",
       " 'soul': 26434,\n",
       " 'actual': 255,\n",
       " 'taste': 28183,\n",
       " 'thanks': 28486,\n",
       " 'excess': 9710,\n",
       " 'went': 31192,\n",
       " 'what': 31220,\n",
       " 'surprising': 27770,\n",
       " 'disappointing': 7739,\n",
       " 'stayed': 27004,\n",
       " 'away': 1837,\n",
       " 'clichés': 4977,\n",
       " 'hindi': 13226,\n",
       " 'cinema': 4806,\n",
       " 'venture': 30476,\n",
       " 'second': 24953,\n",
       " 'outing': 20002,\n",
       " 'gives': 11778,\n",
       " 'stereotype': 27073,\n",
       " 'formulas': 10990,\n",
       " 'vipul': 30681,\n",
       " 'shah': 25277,\n",
       " 'conviction': 5943,\n",
       " 'implausible': 14017,\n",
       " 'blind': 2838,\n",
       " 'robbing': 23959,\n",
       " 'bank': 2061,\n",
       " ...}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepro_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_preprocesed_train = prepro_vectorizer.transform(train_set[\"tokenizada\"])\n",
    "vectors_preprocesed_test = prepro_vectorizer.transform(test_set[\"tokenizada\"]) # Se usa el diccionario de train, si algun token de test no aparece en train no se contará"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las transformaciones de sklearn son como su one-hot encoder (sí de hecho es lo que hacemos con una vectorización binaria), es decir devuelven una matriz sparse (no todos los datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectors_preprocesed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prepro_vectorizer.get_feature_names_out()) # Aquí es donde están ordenadas las apariciones, para poder construir un dataframe a partir de la matriz sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_binary_vectorized = pd.DataFrame(vectors_preprocesed_train.toarray(), columns = prepro_vectorizer.get_feature_names_out())\n",
    "X_test_binary_vectorized = pd.DataFrame(vectors_preprocesed_test.toarray(), columns = prepro_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaargh</th>\n",
       "      <th>aag</th>\n",
       "      <th>aames</th>\n",
       "      <th>aankhen</th>\n",
       "      <th>aapke</th>\n",
       "      <th>aardman</th>\n",
       "      <th>aargh</th>\n",
       "      <th>aaron</th>\n",
       "      <th>...</th>\n",
       "      <th>zsigmond</th>\n",
       "      <th>zu</th>\n",
       "      <th>zucco</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zulu</th>\n",
       "      <th>zuniga</th>\n",
       "      <th>zwick</th>\n",
       "      <th>ème</th>\n",
       "      <th>émigré</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 32000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aaa  aaargh  aag  aames  aankhen  aapke  aardman  aargh  aaron  \\\n",
       "0       0    0       0    0      0        0      0        0      0      0   \n",
       "1       0    0       0    0      0        0      0        0      0      0   \n",
       "2       0    0       0    0      0        0      0        0      0      0   \n",
       "3       0    0       0    0      0        0      0        0      0      0   \n",
       "4       0    0       0    0      0        0      0        0      0      0   \n",
       "...    ..  ...     ...  ...    ...      ...    ...      ...    ...    ...   \n",
       "19995   0    0       0    0      0        0      0        0      0      0   \n",
       "19996   0    0       0    0      0        0      0        0      0      0   \n",
       "19997   0    0       0    0      0        0      0        0      0      0   \n",
       "19998   0    0       0    0      0        0      0        0      0      0   \n",
       "19999   0    0       0    0      0        0      0        0      0      0   \n",
       "\n",
       "       ...  zsigmond  zu  zucco  zucker  zuckerman  zulu  zuniga  zwick  ème  \\\n",
       "0      ...         0   0      0       0          0     0       0      0    0   \n",
       "1      ...         0   0      0       0          0     0       0      0    0   \n",
       "2      ...         0   0      0       0          0     0       0      0    0   \n",
       "3      ...         0   0      0       0          0     0       0      0    0   \n",
       "4      ...         0   0      0       0          0     0       0      0    0   \n",
       "...    ...       ...  ..    ...     ...        ...   ...     ...    ...  ...   \n",
       "19995  ...         0   0      0       0          0     0       0      0    0   \n",
       "19996  ...         0   0      0       0          0     0       0      0    0   \n",
       "19997  ...         0   0      0       0          0     0       0      0    0   \n",
       "19998  ...         0   0      0       0          0     0       0      0    0   \n",
       "19999  ...         0   0      0       0          0     0       0      0    0   \n",
       "\n",
       "       émigré  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "19995       0  \n",
       "19996       0  \n",
       "19997       0  \n",
       "19998       0  \n",
       "19999       0  \n",
       "\n",
       "[20000 rows x 32000 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_binary_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brosnan\n",
       "0    19952\n",
       "1       48\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_binary_vectorized[\"brosnan\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si te parecían poco los 32K tokens puedes ver que hemos considerado alguno \"discutible\" (\"\\_is\\_\",\"\\_the\",\"ème\") que también pueden hacer replantearte una limpieza mayor (quitar esos \"\\_\"). De hecho eso haríamos, ahora volver al principio y relimpiar, pero por temas de tiempo seguimos adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizacion no-binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se tiene en cuenta el número de veces que aparece cada token en el texto correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro_vectorizer = CountVectorizer(binary = False, max_features = 32000) # tengo en cuenta todas las apariciones\n",
    "X_vectors_train_freq = prepro_vectorizer.fit_transform(train_set[\"tokenizada\"]) # Los vamos a manetener como sparse matrix para acelerar los entrenamientos\n",
    "X_vectors_test_freq = prepro_vectorizer.transform(test_set[\"tokenizada\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_vectorized_bow_freq = pd.DataFrame(X_vectors_train_freq.toarray(), columns = prepro_vectorizer.get_feature_names_out())\n",
    "#df_test_vectorized_bow_freq = pd.DataFrame(vectors_preprocesed_test.toarray(), columns = prepro_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver las diferencias busquemos algún token que pueda aparecer más de una vez, como por ejemplo (si acertaste lo tenía preparado), \"yellow\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yellow\n",
       "0    19932\n",
       "1       61\n",
       "2        3\n",
       "5        2\n",
       "3        1\n",
       "7        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_vectorized_bow_freq[\"yellow\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y si lo vemos en el vectorizado binario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yellow\n",
       "0    19932\n",
       "1       68\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_binary_vectorized[\"yellow\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por curiosidad, quien repite \"yellow\", 7 veces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Jag är nyfiken \\x96 Yellow\" is a lot of fun. Like at least one other reviewer, I was, on numerous occasions, laughing out loud. Yellow is energetic, playful, self-aware, explorative. Don\\'t expect Bergman here. This movie is about a youth in the early- to mid-60s in Sweden and about the issues, read *contradictions*, that the nation and the world were facing. At times Yellow appears to be an earnest social-political documentary, with Lena, the main character, and others interviewing both common people and politicians (e.g. Olaf Palme at home). At other times, Yellow seems to parody this kind of documentary. All the while, Yellow acts as a personal documentary exploring Lena\\'s life - her home life, her loves, her political views, her view of herself. She is a complete person \\x96 complex, flawed, contradictory, happy, sad, curious. And placed over all of this is the wonderful additional dimension of the director, Sjöman, and his crew documenting themselves documenting Lena. It is this that, for me, really gives Yellow wings. Not only do they suddenly appear at some very funny times and in some funny ways, reminding the viewer that this is fiction and artifice, but their presence is itself another layer of the film; they are filming themselves filming themselves. I am reminded of a Bjork music video with this same quality \\x96 a music video about the making of a music video, ad infinitum, with each iteration getting weirder and more cartoonish. I think Sjöman may have had something similar in mind. While \"Jag är nyfiken \\x96 Yellow\" may not be everyone\\'s cup of tea, it is certainly intelligent, witty, refreshing, ebullient, and authentic.']\n"
     ]
    }
   ],
   "source": [
    "print(train_set.loc[df_train_vectorized_bow_freq.yellow == 7, \"reseña\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una pena no tener el título porque verías que la película a la que hace referencia es la primera del dataset \"I am a curious yellow\" (no preguntes más, la conozco tanto como tú)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizacion de texto (II): con tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma común de representar las \"features\" de texto en lenguaje natural en un dataframe es utilizar la estadística tf-idf (frecuencia de término-frecuencia inversa de documento) para cada palabra/token, que es un factor de ponderación que podemos usar en lugar de representaciones binarias o de conteo de palabras.\n",
    "\n",
    "*NOTA*: Antes de continuar, un poco de terminología NLP. En NLP se designa como **documento** a cada fragmento de texto a vectorizar o analizar por individual, en nuestro caso cada reseña sería un documento. Y se desgina como **corpus** al conjunto completo de documentos que se tratan de forma agrupada, en nuestro caso el corpus del train es el conjunto total de reseñas del train y el corpus del test sería el conjunto total de reseñas del test. Sigamos.\n",
    "\n",
    "Existen varias formas de realizar la transformación tf-idf, pero en esencia, tf-idf pretende representar el número de veces que una palabra dada aparece en un documento (una reseña de película en nuestro caso) en relación con el número de documentos (reseñas) en el corpus (todas las reseñas de train o de test) en los que aparece la palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por cada token en un texto se hace el cáculo de su tf-idf y eso es lo que se almacena.\n",
    "Para cada texto, pasa por cada token del texo:  \n",
    "    * Calculamos tf -> El número de veces que aparece ese token/El número de tokens que hay en el documento/instancia (en nuestro caso el número de veces que un token aparece en la review que estamos vectorizando)  \n",
    "    $$tf(token_x) = num\\_apariciones\\_token_x$$\n",
    "    * Calculamos la idf -> la inversa de una medida de la frecuencia de aparición del token en todos los textos del dataset/corpus:\n",
    "    $$idf(token_x) = \\log({\\frac{len(dataset)}{df(token_x)}}) + 1,\\text{donde }df(token_x)\\ \\text{es el número de reseñas en las que aparece el token x}$$ \n",
    "    * Y el valor que almacenaríamos en la feature del texto para el token es el producto de los dos valores anteriores. Y digo almacenaríamos porque sklearn aplica dos modificaciones por defecto, una fórmula suavizada y una normalización (una divisón del valor total por la norma del vector resultante (pero a efectos teóricos lo que se está haciendo es una ponderación de los valores en función de las frecuencias de aparición).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo sencillo y luego lo aplicaremos a nuestro train y nuestro test... por supuesto, tiramos de scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fat</th>\n",
       "      <th>is</th>\n",
       "      <th>my</th>\n",
       "      <th>name</th>\n",
       "      <th>ralph</th>\n",
       "      <th>Sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My name is Ralph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.098612</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ralph is fat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ralph</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fat        is        my      name  ralph              Sent\n",
       "0  0.000000  1.405465  2.098612  2.098612    1.0  My name is Ralph\n",
       "1  2.098612  1.405465  0.000000  0.000000    1.0      Ralph is fat\n",
       "2  0.000000  0.000000  0.000000  0.000000    1.0             Ralph"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# TfidfTransformer\n",
    "'''\n",
    "Cuanto mas comun, mas bajo es el TfidfVectorizer\n",
    "'''\n",
    "sent1 = 'My name is Ralph'\n",
    "sent2 = 'Ralph is fat'\n",
    "sent3 = 'Ralph'\n",
    "\n",
    "test = TfidfVectorizer(smooth_idf= False, norm = None) # Para que sea la fórmula anterior, sino usa una para suavizar\n",
    "valores = test.fit_transform([sent1, sent2, sent3])\n",
    "df_test_tfidf = pd.DataFrame(valores.toarray(), columns = test.get_feature_names_out())\n",
    "df_test_tfidf[\"Sent\"] = [sent1,sent2,sent3]\n",
    "df_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4054651081081644"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1*(np.log(3/2)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apliquémoslo a nuestro train y test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizador_tfidf = TfidfVectorizer(max_features = 32000)\n",
    "vec_train_tfidf = prepro_vectorizer.fit_transform(train_set[\"tokenizada\"])\n",
    "vec_test_tfidf = prepro_vectorizer.transform(test_set[\"tokenizada\"])\n",
    "features_tfidf = prepro_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ahora y para ahorrar memoria no lo vamos a expandir a un dataset, de hecho te mostraré que puedes usar las sparse matrix en sklearn directamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde un punto de vista un poco \"grosero\" el tf-idf se podría considerar como una normalización o escalado particular bastante interantes (porque se adapta a las propiedades genéricas del corpus) para un BoW normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorización de texto (III): Con n-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos a cambiar la unidad de referencia, en vez de ser el token vamos a utilizar lo n-gramms (n-gramas).  \n",
    "Un **n-gramma** es una combinación de n palabras/tokens que van seguidas. n = 1 -> token.  \n",
    "Podemos hacer que en vez de contar tokens se cuenten n-grams, y además no sólo un tipo de n-gram, puedo incluir tokens (1-grams), bi-grams (2-grams), tri-gramas (3-grams)... etc y lo que haremos será vectorizar como hemos hecho antes pero sobre los n-gramas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que veas el concepto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is foo bar\n",
      "2-grams\n",
      "('this', 'is')\n",
      "('is', 'foo')\n",
      "('foo', 'bar')\n",
      "###############\n",
      "3-grams\n",
      "('this', 'is', 'foo')\n",
      "('is', 'foo', 'bar')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'this is foo bar'\n",
    "\n",
    "two = ngrams(sentence.split(), 2)\n",
    "three = ngrams(sentence.split(), 3)\n",
    "\n",
    "print(sentence)\n",
    "print(\"2-grams\")\n",
    "for grams in two:\n",
    "    print(grams)\n",
    "print('###############')\n",
    "print(\"3-grams\")\n",
    "for grams in three:\n",
    "    print(grams)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces ahora en vez de considerar los tokens sueltos iremos considerando agrupaciones de 2,3,.., n tokens que aparezcan seguidos (es decir para el ejemplo anterior en el caso de bigramas, 2-gram, n= 2, no  buscaríamos cuánto aparecen \"this\", \"is\",\"foo\" o \"bar\", buscaríamos cuánto aparecen \"this is\", \"is foo\", etc, etc). Te puedes imaginar que esto dispara aún más nuestro número de features en la vectorización y  más si además no sólo me centro en bigramas sino en tokens y bigramas juntos, etc, etc. Como la explosión combinatoria de la regresión polinómica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1]]\n",
      "9\n",
      "dict_keys(['this', 'is', 'foo', 'bar', 'this is', 'is foo', 'foo bar', 'this is foo', 'is foo bar'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bar</th>\n",
       "      <th>foo</th>\n",
       "      <th>foo bar</th>\n",
       "      <th>is</th>\n",
       "      <th>is foo</th>\n",
       "      <th>is foo bar</th>\n",
       "      <th>this</th>\n",
       "      <th>this is</th>\n",
       "      <th>this is foo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bar  foo  foo bar  is  is foo  is foo bar  this  this is  this is foo\n",
       "0    1    1        1   1       1           1     1        1            1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(binary=True,\n",
    "                                   ngram_range=(1, 3)) # Va contar para los tokens originales, para 2-gramms y para 3-gramms\n",
    "\n",
    "vector = ngram_vectorizer.fit_transform([sentence]).toarray()\n",
    "print(vector)\n",
    "print(len(vector[0]))\n",
    "print(ngram_vectorizer.vocabulary_.keys())\n",
    "pd.DataFrame(vector, columns= ngram_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido al uso de memoria, no vamos a aplicarlo a nuestro dataset inicial y no entrenaremos con ello, solo vamos a ver como sería el diccionario o vocabulario que se obtendría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ham least', 1465698), ('made feel physically', 1973172), ('touching although', 3429187), ('band brothers accomplished', 257813), ('lot content stereotypical', 1940033), ('things wells', 3348414), ('webcam cayman islands', 3655741), ('fallon great', 1099349), ('humour breaks scenes', 1590216), ('mark certain audiences', 2029282)]\n"
     ]
    }
   ],
   "source": [
    "import random as rm\n",
    "ngram_vectorizer.fit(train_set[\"tokenizada\"])\n",
    "print(rm.sample([(ngrama,feature_num) for ngrama,feature_num in ngram_vectorizer.vocabulary_.items()],10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3806866\n"
     ]
    }
   ],
   "source": [
    "print(len(ngram_vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ufffff!!! Ese el número de features que obtendríamos de hacer la vectorización aunando tokens, bigramas y trigramas. No lo vamos a hacer, pero con tiempo y recursos podrías planteartelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte IV.2: Selección/Reducción de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos las features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features para vectorización binaria: 32000\n",
      "Features para vectorización no binaria: 32000\n",
      "Features para vectorización tf-idf: 32000\n"
     ]
    }
   ],
   "source": [
    "print(\"Features para vectorización binaria:\", X_train_binary_vectorized.shape[1])\n",
    "print(\"Features para vectorización no binaria:\", X_vectors_train_freq.shape[1])\n",
    "print(\"Features para vectorización tf-idf:\", len(prepro_vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas as 32K porque es el número de entradas en el vocabulario/diccionario que permitimos. Ahora podríamos hacer una reducción de features con los mecanismos que vimos en el sprint anterior (menos el visual, claro), incluyendo la PCA. No lo vamos a hacer, por limitaciones de tiempo, pero puedes ver como aquí sí que aplica una buena reducción de la dimensionalidad (32K features > 20K instancias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que si vamos a hacer es **<u>comentar** (no da para más esta parte introductoria) dos formas específicas de NLP para reducir features que se aplican OJO antes de la vectorización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INCISO: REDUCCION FEATURES EN NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora hemos usado cada \"palabra\" como un token. Pero hay muchas palabras que en realidad están muy relacionadas y puede ser interesante considerarlas como la misma. Por ejemplo: \n",
    "- las formas verbales de un mismo verbo: pienso, pensé, pensasteís (ahora las consideramos como tokens distintos, pero puede que nos interese sólo considerarlo como <pensar>)\n",
    "- Los plurales: casas, casa -> ahora son dos tokens, pero igual quiero considerarlo sólo como una feature (casa)\n",
    "- Los adjetivos: simpático, simpática, simpáticos, simpáticas -> dejarlo en simpatica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos dos formas de hacerlo: stemming y lemmatization\n",
    "- stemming: buscamos la \"raiz\" de la palabra quitando prefijos y sufijos -> prefabricado, fabrica se convierten en fabric\n",
    "- lemmatization: buscamos el \"lemma\", raiz léxica, de la palabra (la palabra original de la que viene, en los verbos por ejemplo su infinitivo) -> corriendo, corrí se convierten en correr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasaremos el Stemming y la Lemmatization (UNO U OTRO) y después haremos la vectorización, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí te dejo el código para que puedas aplicarlo a los datasets (pero en local, es decir en tu ordenador), pero nosotros no vamos a ejecutarlo, recuerda que es antes de la vectorización que quieras aplicar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming [NO EJECUTAR EN LA PLATAFORMA]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has several stemming algorithm implementations. We’ll use the Porter stemmer. Most used:\n",
    "\n",
    "    PorterStemmer\n",
    "    SnowballStemmer\n",
    "\n",
    "Apply a PoterStemmer, vectorize, and train the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer_eng = PorterStemmer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [stemmer_eng.stem(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para español SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr cas play vol vol volv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "plurals = ['corriendo', 'casas', 'playa', 'volando', 'volar', 'volveré']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_eng(row):\n",
    "    return stemmer_eng.stem(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tokenizda_stmmed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tokenizda_stmmed'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m prepro_vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m prepro_vectorizer\u001b[38;5;241m.\u001b[39mfit(train_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizada_stemmed\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m vectors_preprocesed \u001b[38;5;241m=\u001b[39m prepro_vectorizer\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mtrain_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizda_stmmed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Y aquí vendría la parte del test y la conversión a dataframe, si la necesitas y que sólo tienes que adaptar de las secciones anteriores*\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tokenizda_stmmed'"
     ]
    }
   ],
   "source": [
    "\n",
    "train_set[\"tokenizada_stemmed\"] = train_set[\"tokenizada\"].apply(stemming_eng)\n",
    "test_set[\"tokenizada_stemmed\"] = test_set[\"tokenizada\"].apply(stemming_eng)\n",
    "prepro_vectorizer = CountVectorizer(binary=True)\n",
    "prepro_vectorizer.fit(train_set[\"tokenizada_stemmed\"])\n",
    "vectors_preprocesed = prepro_vectorizer.transform(train_set[\"tokenizda_stmmed\"])\n",
    "# Y aquí vendría la parte del test y la conversión a dataframe, si la necesitas y que sólo tienes que adaptar de las secciones anteriores*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization [NO EJECUTAR EN LA PLATAFORMA]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fly dy mule study died agreed owned humbled sized meeting stating siezing itemization sensational traditional reference colonizer plotted\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "La diferencia con el stemming es que la lematización tiene en cuenta la morfología\n",
    "de la palabra, sustituyendola por la raiz, no recortándola. Y no es tan restrictivo como el stemming.\n",
    "Necesita un buen diccionario con mapeos, como wordnet\n",
    "\n",
    "En nltk no hay lematizadores en español. Habria que bajarse algun paquete como pip install es-lemmatizer\n",
    "'''\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'studies',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [lemmatizer.lemmatize(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatized_text(word):\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "train_set[\"tokenizada_lemmatized\"] = train_set[\"tokenizada\"].apply(lemmatized_text)\n",
    "test_set[\"tokenizada_lemmatized\"] = test_set[\"tokenizada\"].apply(lemmatized_text)\n",
    "prepro_vectorizer = CountVectorizer(binary=True)\n",
    "prepro_vectorizer.fit(train_set[\"tokenizada_lemmatized\"])\n",
    "\n",
    "vectors_preprocesed = prepro_vectorizer.transform(train_set[\"tokenizada_lemmatized\"])\n",
    "# Y aquí vendría la parte del test y la conversión a dataframe, si la necesitas y que sólo tienes que adaptar de las secciones anteriores*\n",
    "\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte V: Escoger modelos con Cross_Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tipo de tarea nos vale muy bien una Regresión Logística o un SVM (sí, lo sé) con kernel lineal y debido al número de features los árboles sufren si no controlamos el número de features a considerar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set[target]\n",
    "y_test = test_set[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from time import time\n",
    "\n",
    "feature_sets = {\n",
    "    \"BoW Binaria\": X_train_binary_vectorized,\n",
    "    \"BoW No-Binaria\": X_vectors_train_freq,\n",
    "    \"BoW Tf-idf\": vec_train_tfidf\n",
    "}\n",
    "\n",
    "lr_clf = LogisticRegression(max_iter = 10000)\n",
    "rf_clf = RandomForestClassifier(max_features = 60, random_state= 42)\n",
    "svm_clf =LinearSVC(random_state = 42)\n",
    "\n",
    "\n",
    "modelos = {\n",
    "    \"Regresion Logistica\": lr_clf,\n",
    "    \"SVM\": svm_clf,\n",
    "    \"Random Forest\": rf_clf\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para <BoW Binaria>:\n",
      "\tRegresion Logistica, score: 0.87145, elapsed: 285.61503863334656\n",
      "\tSVM, score: 0.85125, elapsed: 313.0272545814514\n",
      "\tRandom Forest, score: 0.8441999817200859, elapsed: 525.8070628643036\n",
      "Para <BoW No-Binaria>:\n",
      "\tRegresion Logistica, score: 0.87425, elapsed: 7.287132501602173\n",
      "\tSVM, score: 0.85395, elapsed: 12.177056789398193\n",
      "\tRandom Forest, score: 0.8467501542414628, elapsed: 107.13801431655884\n",
      "Para <BoW Tf-idf>:\n",
      "\tRegresion Logistica, score: 0.87425, elapsed: 7.115401268005371\n",
      "\tSVM, score: 0.85395, elapsed: 12.164503574371338\n",
      "\tRandom Forest, score: 0.8467501542414628, elapsed: 107.86866402626038\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for nombre,set in feature_sets.items():\n",
    "    t_zero = time()\n",
    "    print(f\"Para <{nombre}>:\")\n",
    "    for name, model in modelos.items():\n",
    "        #metrica = 1\n",
    "        metrica = np.mean(cross_val_score(model, set, y_train, cv = 5 if \"Random\" not in name else 3, scoring = \"accuracy\"))\n",
    "        print(f\"\\t{name}, score: {metrica}, elapsed: {time() - t_zero}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con la regresión logística y con la vectorización por tf-idf, además es interesante que te fijes en los tiempos para que cuando tengas que trabajar con matrices muy dispersas y ya la tengas con ese tipo en numpy lo hagas directamente y no las transformes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte VI: Ajuste de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo vamos a ajustar el factor de penalización, recuerda que es el inverso del alfa de regularización l1 o l2 o elastic net, es decir a menos más regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=10000),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 0.2, 0.6, 1, 10]}, scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=10000),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 0.2, 0.6, 1, 10]}, scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=10000),\n",
       "             param_grid={'C': [0.1, 0.2, 0.6, 1, 10]}, scoring='accuracy')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"C\": [0.1,0.2,0.6,1,10]\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(lr_clf,\n",
    "                       param_grid = param_grid,\n",
    "                       cv = 5,\n",
    "                       scoring = \"accuracy\"\n",
    "                      )\n",
    "\n",
    "lr_grid.fit(vec_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88045"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      2500\n",
      "           1       0.88      0.90      0.89      2500\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr_grid.best_estimator_.predict(vec_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No está nada mal, ya tenemos el modelo que Showtimes quería, un recall estupendo y una precisión por encima del 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte VII: Analisis de errores/coeficientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tocaría el momento de analizar probabilidades (ya que al ser un clasificador binario los errores de una clase se convierten en asignaciones a la otra, y por lo tanto es más interesante analizar las probabilidades por si hubiera manera de jugar con los umbrales, por ejemplo). Pero como ha salido tan bien, otra cosa que podemos hacer es analizar los pesos para ver qué tokens están influyendo más en la valoración positiva y negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es posible, gracias a que es una regresión logística y sus pesos tienen signo lo cual nos ayuda en ese sentido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aaa', 'aag', ..., 'zwick', 'ème', 'émigré'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>-0.059737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaa</th>\n",
       "      <td>-0.059781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aag</th>\n",
       "      <td>-0.005530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aames</th>\n",
       "      <td>0.011283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aankhen</th>\n",
       "      <td>0.002127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zulu</th>\n",
       "      <td>-0.069833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuniga</th>\n",
       "      <td>-0.058560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zwick</th>\n",
       "      <td>-0.003872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ème</th>\n",
       "      <td>0.005413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>émigré</th>\n",
       "      <td>-0.025723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Importancia\n",
       "aa         -0.059737\n",
       "aaa        -0.059781\n",
       "aag        -0.005530\n",
       "aames       0.011283\n",
       "aankhen     0.002127\n",
       "...              ...\n",
       "zulu       -0.069833\n",
       "zuniga     -0.058560\n",
       "zwick      -0.003872\n",
       "ème         0.005413\n",
       "émigré     -0.025723\n",
       "\n",
       "[32000 rows x 1 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coef = pd.DataFrame(lr_grid.best_estimator_.coef_[0], columns = [\"Importancia\"], index = list(features_tfidf)) # Recuperando la variable donde guardamos el nombre de las features\n",
    "df_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Positivos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "excellent      0.898534\n",
       "perfect        0.808216\n",
       "funniest       0.744412\n",
       "wonderfully    0.720855\n",
       "hilarious      0.658467\n",
       "Name: Importancia, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Tokens Positivos\")\n",
    "df_coef.Importancia.nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Negativos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "worst            -1.382145\n",
       "waste            -1.220939\n",
       "awful            -1.069737\n",
       "disappointment   -0.989780\n",
       "poorly           -0.935525\n",
       "Name: Importancia, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Tokens Negativos\")\n",
    "df_coef.Importancia.nsmallest(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los coeficientes y las palabras se puede construir un diccionario \"sentimental\" que se puede aplicar a otras clasificaciones de texto (de forma que ahora no sustituiremos los tokens por sus valores de conteo o tf-idf sino por ejemplo por estos valores \"sentimentales\") y que además no tengan que ver con el cine, ni nada parecido."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
